{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05_mlops_cloud_deployment.ipynb\n",
    "\n",
    "## Week 5: MLOps Essentials (Docker, Serving, CI/CD, Cloud, Monitoring)\n",
    "\n",
    "### Notebook Overview\n",
    "Welcome to **Week 5**, where you'll bridge the gap between model training and **production deployment**. By the end of this notebook, you will:\n",
    "1. **Containerize** your ML model using Docker.\n",
    "2. Create a **model serving** API (e.g., with FastAPI or Flask).\n",
    "3. Set up a **CI/CD pipeline** to automatically build and test your Docker images.\n",
    "4. **Deploy** the container to a **cloud** environment (AWS, GCP, or Azure).\n",
    "5. Implement **monitoring & logging** to observe your model’s performance in production.\n",
    "\n",
    "**Industry Context**: MLOps is critical at scale—FAANG and other companies use these practices to ensure reliable, reproducible, and maintainable machine learning services. Mastering these skills makes you stand out as a true AI/ML Engineer.\n",
    "\n",
    "#### ADHD-Friendly Notes\n",
    "- We’ve broken tasks down **day-by-day**.\n",
    "- Each day’s tasks have **checklists** and short descriptions, so you can tackle them in small, **manageable chunks**.\n",
    "- **Reward yourself** after completing each day’s tasks (short break, favorite snack, or relaxing activity). This helps sustain focus.\n",
    "\n",
    "---\n",
    "## 1. Monday: Docker Basics & Packaging Your Model\n",
    "\n",
    "### Topics:\n",
    "- **Docker** fundamentals: Dockerfile, building images, running containers.\n",
    "- **Packaging** a simple ML model.\n",
    "\n",
    "### Why This Matters\n",
    "Docker allows you to ship your model and environment together, ensuring consistency across dev, test, and prod environments.\n",
    "\n",
    "### Notebook Tasks:\n",
    "1. **Install Docker** (if not already) and confirm it’s running (`docker --version`).\n",
    "2. Write a **Dockerfile** that:\n",
    "   - Uses a Python base image.\n",
    "   - Installs required libraries (`scikit-learn`, `pandas`, etc.).\n",
    "   - Copies your model/code into the container.\n",
    "   - Specifies a command to run your script.\n",
    "3. **Build & Run** the Docker image locally.\n",
    "\n",
    "#### ADHD Tip\n",
    "- Check off each step as you go. For example:\n",
    "  1. Docker installed ✓\n",
    "  2. Dockerfile created ✓\n",
    "  3. Image built and tested ✓\n",
    "- Use a sticky note or quick to-do app for micro-tracking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Dockerfile (Skeleton)\n",
    "```Dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Create app directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy requirements and install\n",
    "COPY requirements.txt requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy your code into the container\n",
    "COPY . .\n",
    "\n",
    "# Command to run your script\n",
    "CMD [\"python\", \"run_model.py\"]\n",
    "```\n",
    "\n",
    "1. Create a file named `Dockerfile` in your project.\n",
    "2. Create a `requirements.txt` listing dependencies (e.g., scikit-learn==1.0, fastapi==0.85, etc.).\n",
    "3. In the terminal, run:\n",
    "```\n",
    "docker build -t my_ml_model:latest .\n",
    "docker run -p 8080:8080 my_ml_model:latest\n",
    "```\n",
    "\n",
    "**Your Observations**:\n",
    "- *(Document any issues encountered with dependencies. Note how you resolved them.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tuesday: Model Serving with FastAPI (or Flask)\n",
    "\n",
    "### Topics:\n",
    "- Creating a **REST API** for inference.\n",
    "- Exposing an endpoint (e.g., `/predict`) that accepts JSON input.\n",
    "\n",
    "### Why This Matters\n",
    "Clients or other services typically call a REST endpoint to get predictions. This is the standard approach in modern microservice architectures.\n",
    "\n",
    "### Notebook Tasks:\n",
    "1. **Install** FastAPI (e.g., `pip install fastapi uvicorn`).\n",
    "2. Create a simple server script (`main.py`) that:\n",
    "   - Loads a pre-trained model (e.g., scikit-learn .pkl file or a PyTorch model).\n",
    "   - Defines a `/predict` endpoint that takes input data.\n",
    "   - Returns predictions.\n",
    "3. **Test** locally via `curl` or a small Python client.\n",
    "\n",
    "#### ADHD Tip\n",
    "- Write down a short code snippet for the `/predict` endpoint.\n",
    "- Test in small increments (e.g., start the server, send one request, confirm the response, repeat).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example FastAPI Skeleton\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Define input schema\n",
    "class PredictRequest(BaseModel):\n",
    "    feature1: float\n",
    "    feature2: float\n",
    "\n",
    "# Load your model\n",
    "model = joblib.load(\"./my_model.pkl\")\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(data: PredictRequest):\n",
    "    # Convert input to the right shape\n",
    "    X = [[data.feature1, data.feature2]]\n",
    "    prediction = model.predict(X)\n",
    "    return {\"prediction\": prediction[0]}\n",
    "```\n",
    "\n",
    "You’d typically run this with `uvicorn main:app --host 0.0.0.0 --port 8080`.\n",
    "\n",
    "**Your Observations**:\n",
    "- *(Write any notes about how you tested this or how you’d handle errors.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wednesday: CI/CD Pipeline (GitHub Actions or Similar)\n",
    "\n",
    "### Topics:\n",
    "- **Continuous Integration**: Automated tests whenever you push code.\n",
    "- **Continuous Delivery/Deployment**: Building and pushing Docker images automatically.\n",
    "\n",
    "### Why This Matters\n",
    "CI/CD ensures your code is always tested, your Docker image is consistently built, and your ML service can be deployed with minimal manual intervention.\n",
    "\n",
    "### Notebook Tasks:\n",
    "1. Create a **GitHub Actions** workflow file (e.g., `.github/workflows/docker-build.yml`).\n",
    "2. Make it **build** your Docker image on every push.\n",
    "3. Optionally run **unit tests** (like a small `test_model.py`).\n",
    "4. Configure it to **push** the image to Docker Hub or GitHub Container Registry.\n",
    "\n",
    "#### ADHD Tip\n",
    "- Keep your CI steps minimal at first (just a Docker build and basic lint/test).\n",
    "- Add complexity gradually (e.g., integration tests) once you have the basics down.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example GitHub Actions Workflow (Skeleton)\n",
    "```yaml\n",
    "name: CI/CD Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [\"main\"]\n",
    "\n",
    "jobs:\n",
    "  build-and-push:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v2\n",
    "\n",
    "      - name: Set up QEMU\n",
    "        uses: docker/setup-qemu-action@v2\n",
    "\n",
    "      - name: Set up Docker Buildx\n",
    "        uses: docker/setup-buildx-action@v2\n",
    "\n",
    "      - name: Build and push Docker image\n",
    "        uses: docker/build-push-action@v2\n",
    "        with:\n",
    "          context: .\n",
    "          file: ./Dockerfile\n",
    "          push: true\n",
    "          tags: your-dockerhub-username/your-repo:latest\n",
    "          # Optionally specify more tags or versions\n",
    "```\n",
    "\n",
    "**Your Observations**:\n",
    "- *(Note how you had to store Docker Hub credentials as GitHub secrets. Document each step to avoid confusion!)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Thursday: Cloud Deployment\n",
    "\n",
    "### Topics:\n",
    "- Deploying to **AWS** (ECS, EKS, or Elastic Beanstalk), or **GCP** (Cloud Run), or Azure.\n",
    "- Setting up your container to run on a managed service.\n",
    "- Exposing a public endpoint.\n",
    "\n",
    "### Why This Matters\n",
    "Production ML systems need to be publicly accessible (or accessible within a company’s network). Cloud deployment is the standard approach to scale and manage.\n",
    "\n",
    "### Notebook Tasks:\n",
    "1. Choose **one cloud** platform (AWS, GCP, or Azure) to keep it simple.\n",
    "2. Push your Docker image to a registry (AWS ECR, GCP GCR, or Docker Hub if you prefer).\n",
    "3. Use a managed service to **run** your container (e.g., AWS ECS or GCP Cloud Run).\n",
    "4. Validate you can call the **`/predict`** endpoint from outside.\n",
    "\n",
    "#### ADHD Tip\n",
    "- Focus on just **one** method of deployment. Don’t get lost in the variety of options.\n",
    "- Write down each step (1. create a service, 2. configure CPU/memory, 3. open port 8080, etc.). Check them off as you succeed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: AWS ECS Fargate (Short Overview)\n",
    "1. Create an **ECR repository**.\n",
    "2. Use `docker login` to authenticate with ECR.\n",
    "3. Tag and **push** your local image to ECR.\n",
    "4. In the AWS console, create an **ECS cluster** (Fargate), define a **task** that references your ECR image.\n",
    "5. Expose port 8080, set up a security group to allow inbound traffic.\n",
    "6. **Run** the task, get the public IP or load balancer URL.\n",
    "7. Test with `curl http://<YOUR_ECS_SERVICE_URL>:8080/predict`.\n",
    "\n",
    "**Your Observations**:\n",
    "- *(Describe any difficulties or clarifications needed. For instance, IAM roles, region settings, or VPC configurations.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Friday: Monitoring & Logging\n",
    "\n",
    "### Topics:\n",
    "- **Logging** (stdout, CloudWatch, Stackdriver, etc.).\n",
    "- **Basic metrics** (request counts, latency, error rates).\n",
    "- Potential introduction to **model performance monitoring** (concept drift, data drift, etc.).\n",
    "\n",
    "### Why This Matters\n",
    "Monitoring ensures you know how your model performs and if anything breaks. In production, unseen issues (e.g., changes in data distribution) can degrade performance silently.\n",
    "\n",
    "### Notebook Tasks:\n",
    "1. Enable **logs** on your cloud service or container orchestrator (e.g., CloudWatch logs for ECS, Stackdriver for GCP).\n",
    "2. Print logs for each request in your FastAPI/Flask code.\n",
    "3. (Optional) Store prediction inputs/outputs somewhere (if privacy/compliance rules allow) for later analysis.\n",
    "4. Summarize how you’d detect data drift or degrade in model performance over time.\n",
    "\n",
    "#### ADHD Tip\n",
    "- Write down a small function or snippet that logs the request time, client IP, request payload, etc.\n",
    "- Keep it simple. Detailed logging frameworks can be added later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Logging Snippet (FastAPI)\n",
    "```python\n",
    "import logging\n",
    "from fastapi import FastAPI, Request\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def log_requests(request: Request, call_next):\n",
    "    logger.info(f\"Incoming request {request.url} from {request.client}\")\n",
    "    response = await call_next(request)\n",
    "    logger.info(f\"Response status {response.status_code}\")\n",
    "    return response\n",
    "```\n",
    "\n",
    "**Your Observations**:\n",
    "- *(Document how you see these logs in CloudWatch or local logs. Are they easy to read?)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekend: End-to-End MLOps Mini-Project\n",
    "\n",
    "**Objective**: Combine the entire pipeline into a small, but complete example.\n",
    "1. **Train or reuse** an ML model (could be a scikit-learn model from earlier weeks).\n",
    "2. **Containerize** with Docker.\n",
    "3. **Serve** with FastAPI.\n",
    "4. **Set up** CI/CD on GitHub Actions to build/push.\n",
    "5. **Deploy** to AWS (or GCP/Azure) so you have a public endpoint.\n",
    "6. **Implement** logging and check your logs as you make requests.\n",
    "\n",
    "**Notes**:\n",
    "- Keep it small. Even a simple Iris classifier is enough for demonstration.\n",
    "- Document each step meticulously.\n",
    "- This project will be a significant piece in your portfolio.\n",
    "\n",
    "### ADHD Tip\n",
    "- Break the end-to-end pipeline into micro-steps (Docker build, test locally, push to registry, create ECS service, etc.). Check each off.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Words on Week 5\n",
    "\n",
    "**Congratulations!** You’ve just covered the **core MLOps pipeline**:\n",
    "- Docker for containerization.\n",
    "- Model serving with an API framework.\n",
    "- CI/CD for automated building & testing.\n",
    "- Cloud deployment for production hosting.\n",
    "- Monitoring & logging to keep track of model health.\n",
    "\n",
    "These steps are **industry must-haves** for any ML engineer or data scientist working in production.\n",
    "\n",
    "### Coming Next: Week 6\n",
    "We’ll look at **Large-Scale & Distributed Systems** and how to handle big data and advanced deployment scenarios.\n",
    "\n",
    "### ADHD Reminder\n",
    "- Reflect on all you’ve learned. Reward yourself for completing these big tasks.\n",
    "- Revisit any challenging steps in short, focused sessions. Small sprints + frequent breaks = success.\n",
    "\n",
    "## End of Week 5 Notebook\n",
    "---\n",
    "You’re ready to **ship** ML models like a pro!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
