{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c2a3ce9",
   "metadata": {},
   "source": [
    "# 02_classical_ml_system_design.ipynb\n",
    "\n",
    "## Week 2: Classical Machine Learning & Intro to System Design Fundamentals\n",
    "\n",
    "### Notebook Overview\n",
    "This notebook covers:\n",
    "- **Supervised Learning (Regression & Classification)**: Linear Regression, Ridge, Lasso, Logistic Regression, Decision Trees, Random Forests, SVM\n",
    "- **Hyperparameter Tuning**: GridSearchCV, RandomizedSearchCV\n",
    "- **Unsupervised Learning**: K-Means, PCA\n",
    "- **Basic ML System Design**: Data ingestion, feature store concept, offline vs. online training, model hosting\n",
    "\n",
    "**By the end of this notebook, you should be able to:**\n",
    "1. Implement classical ML algorithms in scikit-learn.\n",
    "2. Evaluate models with appropriate metrics and tune hyperparameters.\n",
    "3. Understand basic ML system design considerations for scaling.\n",
    "4. Complete a mini-project pipeline that includes data loading, cleaning, model training, and hyperparameter tuning.\n",
    "\n",
    "---\n",
    "## 1. Monday: Supervised Learning (Regression)\n",
    "\n",
    "### Topics:\n",
    "- **Linear Regression** (Ordinary Least Squares)\n",
    "- **Regularization** (Ridge, Lasso)\n",
    "\n",
    "### Notebook Tasks:\n",
    "1. Use the **California Housing** dataset (or Boston Housing if available) to demonstrate regression.\n",
    "2. Compare MSE, MAE, and R² metrics.\n",
    "3. Observe the effects of **regularization**.\n",
    "\n",
    "---\n",
    "## 2. Tuesday: Supervised Learning (Classification)\n",
    "\n",
    "### Topics:\n",
    "- **Logistic Regression**, **Decision Trees**, **Random Forests**, **SVM**\n",
    "- Performance metrics: Accuracy, Precision, Recall, F1, Confusion Matrix\n",
    "\n",
    "### Notebook Tasks:\n",
    "1. Use **Iris** or **Titanic** dataset for classification.\n",
    "2. Train at least two different classifiers.\n",
    "3. Evaluate each with relevant metrics; visualize confusion matrices.\n",
    "\n",
    "---\n",
    "## 3. Wednesday: Hyperparameter Tuning & Model Selection\n",
    "\n",
    "### Topics:\n",
    "- **GridSearchCV**, **RandomizedSearchCV**, cross-validation\n",
    "- Model selection & avoiding overfitting\n",
    "\n",
    "### Notebook Tasks:\n",
    "1. Choose one model (e.g., Random Forest or SVM) and apply `GridSearchCV`.\n",
    "2. Compare performance with default hyperparameters vs. tuned hyperparameters.\n",
    "3. Document the best params and final performance.\n",
    "\n",
    "---\n",
    "## 4. Thursday: Unsupervised Learning & Basic ML System Design\n",
    "\n",
    "### Topics:\n",
    "- **K-Means** clustering\n",
    "- **PCA** for dimensionality reduction\n",
    "- **Basic ML system design**: data ingestion, feature engineering, model hosting (conceptual)\n",
    "\n",
    "### Notebook Tasks:\n",
    "1. Implement K-Means on a small dataset (e.g., Iris ignoring labels).\n",
    "2. Use PCA for visualization in 2D or 3D.\n",
    "3. Write a short outline about how you’d design a system if data were huge (streaming or partial fitting).\n",
    "\n",
    "---\n",
    "## 5. Friday: Mini-Project – Classical ML Pipeline\n",
    "\n",
    "### Objective\n",
    "Combine a small end-to-end pipeline:\n",
    "1. Data Loading & Cleaning\n",
    "2. Feature Engineering (optional, if relevant)\n",
    "3. Model Training (Regression or Classification)\n",
    "4. Hyperparameter Tuning\n",
    "5. Model Evaluation\n",
    "6. Brief mention of how you’d **deploy** or **serve** this model in a real-world system.\n",
    "\n",
    "### Industry Context\n",
    "- This pipeline reflects a typical scenario where data is prepared, models are trained, and eventually served to production.\n",
    "- At scale, teams must consider data versioning, automation (CI/CD), and monitoring.\n",
    "\n",
    "---\n",
    "## 6. Weekend: Consolidation\n",
    "- Review the classical ML techniques and ensure all code runs smoothly.\n",
    "- Solidify understanding of hyperparameter tuning.\n",
    "- **ADHD Tip**: Break tasks down into small chunks each day. Celebrate each milestone!\n",
    "\n",
    "### Next Steps Preview\n",
    "In **Week 3**, we’ll shift focus to **Deep Learning** basics with an intro to neural networks, CNNs, and best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484b7d23",
   "metadata": {},
   "source": [
    "## Practical Implementation Sections\n",
    "\n",
    "Below, you'll find skeleton code cells for each day of the week. Feel free to split or reorganize them as you see fit. Insert your own code, results, visualizations, and notes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e1b80",
   "metadata": {},
   "source": [
    "### 1. Monday: Regression (Linear, Ridge, Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03659a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 1.1 Load California Housing (or Boston Housing) dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "y = housing.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TODO: 1.2 Train LinearRegression, Ridge, Lasso and compare metrics\n",
    "models = {\n",
    "    'Linear': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.1)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{name} -> MSE: {mse:.3f}, MAE: {mae:.3f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0be88a",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- TODO: Write notes on which model performed best, how regularization impacted results, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0836c55",
   "metadata": {},
   "source": [
    "### 2. Tuesday: Classification (Logistic Regression, Decision Tree, Random Forest, SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ab5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 2.1 Load Iris or Titanic dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TODO: 2.2 Train multiple classifiers\n",
    "classifiers = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=200),\n",
    "    'DecisionTree': DecisionTreeClassifier(),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'SVM': SVC()\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(f\"{name} Accuracy: {acc:.3f}\")\n",
    "\n",
    "# TODO: 2.3 Choose one model and visualize the confusion matrix\n",
    "chosen_model = classifiers['RandomForest']\n",
    "preds = chosen_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.title(\"Confusion Matrix - RandomForest\")\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc229b4",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- TODO: Compare results among classifiers.\n",
    "- Which performed best? Why do you think so?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa3fa9",
   "metadata": {},
   "source": [
    "### 3. Wednesday: Hyperparameter Tuning & Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa1094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 3.1 GridSearchCV or RandomizedSearchCV example\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 2, 5]\n",
    "}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Params:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "print(\"Test Accuracy with tuned params:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1ba1dc",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "- Write about how cross-validation works.\n",
    "- Document best params and any noticeable improvements over the default model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad83049",
   "metadata": {},
   "source": [
    "### 4. Thursday: Unsupervised Learning & Basic ML System Design (Conceptual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4333b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 4.1 K-Means clustering with Iris (ignoring labels)\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Let's reuse the Iris data but drop the labels for clustering\n",
    "X_iris = iris.data\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X_iris)\n",
    "\n",
    "print(\"Cluster labels:\", kmeans_labels[:10])\n",
    "\n",
    "# 4.2 PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_iris)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=kmeans_labels, cmap='viridis')\n",
    "plt.title(\"K-Means Clusters in PCA-Reduced Space\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n",
    "\n",
    "# TODO: 4.3 Basic ML system design outline\n",
    "system_design_notes = \"\"\"\n",
    "Imagine we have a huge streaming dataset. We might:\n",
    "- Use a message queue (e.g., Kafka) to handle incoming data.\n",
    "- Store raw data in a data lake (S3, HDFS).\n",
    "- Perform feature engineering offline, then train in a distributed environment (Spark, Dask, or HPC cluster).\n",
    "- Deploy the trained model behind a microservice (FastAPI, Docker).\n",
    "- Log requests and feedback for continuous updates.\n",
    "\"\"\"\n",
    "print(system_design_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270e75a7",
   "metadata": {},
   "source": [
    "**Conceptual Discussion**:\n",
    "- Summarize how you’d handle data ingestion if the dataset were 100 GB or coming in real-time.\n",
    "- Briefly discuss partial fitting or online learning for extremely large data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d21c21a",
   "metadata": {},
   "source": [
    "### 5. Friday: Mini-Project – Classical ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b9f016",
   "metadata": {},
   "source": [
    "**Objective**: Combine data loading, cleaning, feature engineering (optional), model training, hyperparameter tuning, and evaluation into one workflow.\n",
    "\n",
    "**Suggested Steps**:\n",
    "1. Choose a dataset (Iris, Titanic, or another interesting one).\n",
    "2. Clean data if necessary.\n",
    "3. Split into train/test.\n",
    "4. Train 1–2 models.\n",
    "5. Tune hyperparameters.\n",
    "6. Evaluate final model(s).\n",
    "7. Briefly mention deployment/serving ideas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fb710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 5.1 Example skeleton code for a mini-project pipeline\n",
    "def run_classical_ml_pipeline(dataset_df):\n",
    "    # 1. Data cleaning\n",
    "    # 2. Feature engineering\n",
    "    # 3. Train/test split\n",
    "    # 4. Modeling\n",
    "    # 5. Hyperparameter tuning\n",
    "    # 6. Evaluation\n",
    "    pass\n",
    "\n",
    "# fill in with your own logic\n",
    "print(\"Mini-project pipeline placeholder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02034ae4",
   "metadata": {},
   "source": [
    "### Industry Context\n",
    "In real projects, you’ll often have multiple data sources, need to track data versions, and iterate rapidly. This pipeline forms the foundation for production-level ML systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc801f8",
   "metadata": {},
   "source": [
    "## 6. Weekend: Consolidation\n",
    "- **Review** classical ML techniques.\n",
    "- Ensure your code is clean and well-documented.\n",
    "- Reflect on hyperparameter tuning outcomes.\n",
    "- **ADHD Tip**: Keep tasks small and well-defined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a715e5",
   "metadata": {},
   "source": [
    "## Additional Resources (Optional)\n",
    "- [scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "- [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (A. Géron)](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n",
    "- [System Design for ML (various blog posts & tutorials)](https://github.com/puncsky/system-design)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2786b08c",
   "metadata": {},
   "source": [
    "# End of Week 2 Notebook\n",
    "\n",
    "---\n",
    "In **Week 3**, we’ll dive into **Deep Learning** fundamentals, building your first neural networks in TensorFlow or PyTorch!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
