{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_adv_deep_learning.ipynb\n",
    "\n",
    "## Week 4: Advanced Deep Learning & Sequence Models\n",
    "\n",
    "### Notebook Overview\n",
    "This notebook aims to expand on deep learning fundamentals by introducing:\n",
    "1. **RNNs, LSTM, GRU** (Monday)\n",
    "2. **NLP & Transformers** (Tuesday)\n",
    "3. **Transfer Learning** (Wednesday)\n",
    "4. **Optional Advanced Topics** (GANs, Autoencoders, RL) (Thursday)\n",
    "5. **Project Review & Reflection** (Friday)\n",
    "6. **Weekend**: Extend or create a new project focusing on NLP or sequence modeling.\n",
    "\n",
    "By the end of this week, you’ll have:\n",
    "- Gained practical experience with RNN-based models for time-series or text.\n",
    "- Explored modern **Transformer architectures** (e.g., BERT, GPT) for NLP tasks.\n",
    "- Learned about **transfer learning** (vision/NLP) and how to fine-tune pretrained models.\n",
    "- (Optionally) experimented with **GANs**, **autoencoders**, or **RL** if time permits.\n",
    "\n",
    "**Industry Context**: These advanced techniques power state-of-the-art solutions in language understanding, translation, question answering, image recognition, and generative AI.\n",
    "\n",
    "---\n",
    "## 1. Monday: RNN, LSTM, GRU Basics\n",
    "\n",
    "### Topics:\n",
    "- How Recurrent Neural Networks handle sequential data.\n",
    "- **LSTM** (Long Short-Term Memory) and **GRU** (Gated Recurrent Unit) architectures.\n",
    "- Applications: time-series forecasting, text generation.\n",
    "\n",
    "### Notebook Tasks:\n",
    "1. **Review** key RNN concepts (unrolled computation, hidden states, vanishing gradients).\n",
    "2. **Implement** a simple RNN or LSTM for a small sequence problem (e.g., text generation or time-series).\n",
    "3. **Compare** performance of vanilla RNN vs. LSTM/GRU.\n",
    "\n",
    "### Why This Matters\n",
    "RNN-based models excel at processing sequences. LSTMs and GRUs mitigate the vanishing gradient problem, enabling learning from longer contexts.\n",
    "\n",
    "---\n",
    "## 2. Tuesday: NLP & Transformer Basics\n",
    "\n",
    "### Topics:\n",
    "- Word embeddings (Word2Vec, GloVe) vs. modern Transformer embeddings.\n",
    "- **Attention mechanism**, BERT, GPT.\n",
    "- Practical NLP tasks (sentiment analysis, text classification).\n",
    "\n",
    "### Notebook Tasks:\n",
    "1. **Quick Demo** of a Transformer-based model (using Hugging Face Transformers) on a small text classification task.\n",
    "2. **Compare** performance/time to a standard LSTM approach.\n",
    "3. **Write** a short explanation of attention and why Transformers are powerful.\n",
    "\n",
    "### Industry Context\n",
    "Transformers now dominate NLP, powering chatbots, search ranking, content moderation, and more. Understanding how they work is crucial for modern AI roles.\n",
    "\n",
    "---\n",
    "## 3. Wednesday: Transfer Learning\n",
    "\n",
    "### Topics:\n",
    "- Pretrained CNNs (ResNet, VGG, MobileNet) for vision.\n",
    "- Fine-tuning or feature extraction.\n",
    "- Pretrained NLP models (BERT, GPT) for text tasks.\n",
    "\n",
    "### Notebook Tasks:\n",
    "1. **Fine-tune** a pretrained CNN on a small custom dataset (e.g., 2–3 classes of images).\n",
    "2. **Alternatively**, fine-tune BERT on a small text classification dataset (IMDB or a small custom set).\n",
    "3. Document **best practices** for learning rate scheduling, layer freezing, etc.\n",
    "\n",
    "### Observations\n",
    "Transfer learning drastically reduces training time and data requirements, leveraging knowledge from large-scale pretrained models.\n",
    "\n",
    "---\n",
    "## 4. Thursday: Advanced Techniques (Optional Deep Dive)\n",
    "\n",
    "### Topics (Choose One):\n",
    "1. **GANs (Generative Adversarial Networks)**: Learn how a generator and discriminator compete.\n",
    "2. **Autoencoders**: Dimensionality reduction, anomaly detection.\n",
    "3. **Reinforcement Learning**: Q-learning, policy gradients (high-level overview).\n",
    "\n",
    "### Notebook Tasks:\n",
    "1. Build a **simple GAN** on MNIST (if curious about generative models).\n",
    "2. Or implement a **basic autoencoder** for compression or anomaly detection.\n",
    "3. Or explore a minimal **RL** environment (OpenAI Gym) if that interests you.\n",
    "\n",
    "### Why This Matters\n",
    "Exploring these areas broadens your AI toolkit. You see how generative models or RL can solve different classes of problems.\n",
    "\n",
    "---\n",
    "## 5. Friday: Project Review & Reflection\n",
    "\n",
    "### Objective\n",
    "- Review all advanced techniques you experimented with (RNN/LSTM, Transformers, transfer learning, optional advanced method).\n",
    "- Reflect on successes, challenges, and next steps.\n",
    "\n",
    "### Notebook Tasks:\n",
    "1. Summarize **key takeaways** from the week’s implementations.\n",
    "2. Clean up your code/notebook sections for clarity.\n",
    "3. Plan how you might integrate these techniques into an end-to-end scenario.\n",
    "\n",
    "### Industry Context\n",
    "Future real-world projects may combine multiple deep learning methods (e.g., CNN for images + LSTM for textual metadata). Understanding these building blocks is crucial.\n",
    "\n",
    "---\n",
    "## Weekend: Project Extension or New Project\n",
    "- Choose either to **extend** your existing project (e.g., add a Transformer-based NLP component or an LSTM-based time series module).\n",
    "- Or **start fresh**: build a small end-to-end pipeline around a new advanced technique.\n",
    "- **ADHD Tip**: Focus on a single method that excites you (Transformers or LSTM or Transfer Learning). Keep tasks small and track progress daily.\n",
    "\n",
    "### Next Steps Preview\n",
    "In **Week 5**, you'll dive into **MLOps** essentials (Docker, serving, cloud deployment), bridging the gap between modeling and production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Implementation Sections\n",
    "Below, we provide skeleton code cells for each major topic (RNN/LSTM, Transformers, Transfer Learning, etc.). Fill them out as you progress through the week.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Monday: RNN, LSTM, GRU Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes & Concepts**:\n",
    "- *(Summarize how an RNN unrolls over time. Highlight issues like vanishing gradients.)*\n",
    "- *(Explain how LSTM/GRU gates help retain information over longer sequences.)*\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example: LSTM for a toy sequence problem (e.g., sinusoid prediction)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=50, num_layers=1):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        # out shape: (batch, seq_len, hidden_size)\n",
    "        out = self.fc(out[:, -1, :])  # take last time step\n",
    "        return out\n",
    "\n",
    "# You would then create data, train, etc. Pseudocode:\n",
    "print(\"Simple LSTM model defined. Fill in your training loop, data creation, etc.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Observations**:\n",
    "- *(Document performance, how many epochs needed, how LSTM compares to a vanilla RNN.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tuesday: NLP & Transformer Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**:\n",
    "1. Install `transformers` via `pip install transformers` if not installed.\n",
    "2. Load a small dataset (e.g., **IMDB** or **Yelp**) or a sample from **Hugging Face Datasets**.\n",
    "3. Fine-tune a small model like **DistilBERT** on a text classification task.\n",
    "4. Compare with an LSTM-based model (optional if time).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example: Hugging Face Transformers quick demo (text classification)\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "print(\"Example skeleton for loading a pre-trained transformer.\")\n",
    "print(\"You can fill in actual dataset loading & training logic.\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Pseudocode for dataset:\n",
    "# from datasets import load_dataset\n",
    "# imdb_dataset = load_dataset('imdb')\n",
    "# # Preprocess data, tokenize, etc.\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     evaluation_strategy='epoch',\n",
    "#     num_train_epochs=1,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=imdb_dataset['train'],\n",
    "#     eval_dataset=imdb_dataset['test']\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "# # Evaluate and observe results.\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Notes**:\n",
    "- *(Why do Transformers use attention? How does that help capture long-range dependencies?)*\n",
    "- *(Performance difference vs. LSTM if you tried both?)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Wednesday: Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**:\n",
    "1. For computer vision: load a **pretrained CNN** (e.g., ResNet50) from `torchvision.models`, freeze some layers, and fine-tune on a small image dataset.\n",
    "2. Alternatively (or additionally), for NLP: fine-tune BERT on your text classification dataset.\n",
    "3. Document your approach: which layers do you freeze? How many epochs?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example: Transfer Learning with ResNet in PyTorch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "# Freeze layers\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final FC layer to match your target classes\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_ftrs, 2)  # Example: 2 classes\n",
    "\n",
    "print(\"ResNet with final layer replaced.\")\n",
    "print(\"Next: fine-tune the final layer(s) on your dataset.\")\n",
    "\n",
    "# TODO: Data loading, training loop, etc.\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Observations**:\n",
    "- *(Was transfer learning faster than training from scratch? Did you get better performance?)*\n",
    "- *(If you used a pretrained NLP model, note how many epochs until convergence.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Thursday: Advanced Techniques (Optional Deep Dive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose 1**:\n",
    "1. **GAN**: Implement a small DCGAN on MNIST.\n",
    "2. **Autoencoder**: Build a basic autoencoder for dimensionality reduction or anomaly detection.\n",
    "3. **Reinforcement Learning**: Minimal example with OpenAI Gym.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example: Simple Autoencoder Skeleton\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        decoded = decoded.view(-1, 1, 28, 28)\n",
    "        return decoded\n",
    "\n",
    "print(\"Basic Autoencoder structure. Fill in training logic on MNIST.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection**:\n",
    "- *(What did you learn from implementing a GAN/autoencoder/RL environment?)*\n",
    "- *(Where do you see this technique used in industry?)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Friday: Project Review & Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**:\n",
    "- Summarize each advanced technique you covered:\n",
    "  1. RNN/LSTM/GRU\n",
    "  2. Transformers & NLP\n",
    "  3. Transfer learning\n",
    "  4. (Optional) GAN/AE/RL\n",
    "- Note any challenges you faced and how you overcame them.\n",
    "- Polish your notebooks, code, and markdown explanations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentation Tips**:\n",
    "- Provide short **intuition** for each technique.\n",
    "- Include **graphs** of training curves or sample outputs (e.g., generated images from a GAN, reconstructed images from an autoencoder).\n",
    "- Make sure your code is **reproducible** and well-commented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekend: Extend or New Project\n",
    "\n",
    "**Suggestion**: Choose one advanced technique (LSTM-based sequence, Transformer-based NLP, or advanced CV) and build a minimal end-to-end pipeline. If you’d rather start fresh, pick a dataset and apply the new techniques from scratch.\n",
    "\n",
    "**ADHD Tip**: Keep tasks small—write a short list of steps (1) data ingest, (2) model define, (3) train loop, (4) evaluate, etc. Check them off as you go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Week 4 Notebook\n",
    "\n",
    "---\n",
    "Great job diving deeper into advanced deep learning topics! In **Week 5**, you’ll focus on **MLOps Essentials**—how to deploy and serve your models in production.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
