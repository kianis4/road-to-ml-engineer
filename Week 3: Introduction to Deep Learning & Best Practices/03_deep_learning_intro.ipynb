{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac31f9d",
   "metadata": {},
   "source": [
    "# 03_deep_learning_intro.ipynb\n",
    "\n",
    "## Week 3: Introduction to Deep Learning & Best Practices\n",
    "\n",
    "### Notebook Overview\n",
    "This notebook is structured to help you:\n",
    "1. Understand **Neural Network Fundamentals** (Monday).\n",
    "2. Get comfortable with a **Deep Learning Framework** (PyTorch or TensorFlow) & build an MLP for MNIST (Tuesday).\n",
    "3. Implement a **CNN for image classification** (Wednesday).\n",
    "4. Explore **regularization & best practices** (Thursday).\n",
    "5. Complete a **mini-project CNN showcase** (Friday).\n",
    "6. Over the **Weekend**, review & consolidate your learnings.\n",
    "\n",
    "By the end of this week, you'll have:\n",
    "- Built your first neural networks (MLP, CNN) with a popular framework.\n",
    "- Learned about optimization basics (SGD, Adam, etc.), overfitting vs. underfitting, and ways to mitigate.\n",
    "- Gained hands-on experience with experiment tracking.\n",
    "- Prepared a mini-project showcasing a CNN from start to finish.\n",
    "\n",
    "---\n",
    "## 1. Monday: Neural Net Fundamentals\n",
    "\n",
    "### Topics:\n",
    "- Perceptron model, activation functions (ReLU, Sigmoid, Tanh)\n",
    "- Forward and backpropagation concepts\n",
    "- Gradient descent (SGD) basics\n",
    "\n",
    "### Notebook Tasks:\n",
    "1. **Key Concepts**: Write down your own definitions and intuitive explanations of a perceptron, forward pass, backprop.\n",
    "2. (Optional) **From-scratch example**: Implement a small forward pass in pure Python or illustrate the math.\n",
    "\n",
    "### Why This Matters\n",
    "Neural networks are the backbone of deep learning. Understanding the fundamentals (especially how forward/backprop works) helps you debug, tune, and architect more complex models.\n",
    "\n",
    "---\n",
    "## 2. Tuesday: Framework Setup & MLP for MNIST\n",
    "\n",
    "### Topics:\n",
    "- PyTorch or TensorFlow environment setup\n",
    "- Building a simple MLP (multi-layer perceptron) for MNIST classification\n",
    "- Data loading, training loop, evaluating accuracy\n",
    " \n",
    "### Notebook Tasks:\n",
    "1. **Install** PyTorch or TensorFlow (if not already installed).\n",
    "2. **Load MNIST** dataset (or Fashion-MNIST if you want a slightly different challenge).\n",
    "3. **Build** a small MLP, define a loss function (cross-entropy), and optimizer (SGD or Adam).\n",
    "4. **Train** the network, track training/validation accuracy.\n",
    "5. **Interpret** results in a markdown cell.\n",
    "\n",
    "### Industry Context\n",
    "MNIST might seem basic, but it's a common starting point for understanding CNNs and debugging training loops.\n",
    "\n",
    "---\n",
    "## 3. Wednesday: CNN for Image Classification\n",
    "\n",
    "### Topics:\n",
    "- Convolutional Neural Networks (CNNs): Convolution, pooling, typical architectures\n",
    "- Comparing CNN performance to MLP for image tasks\n",
    "- Basic debugging of CNN training\n",
    "\n",
    "### Notebook Tasks:\n",
    "1. **Define** a CNN with a few convolution + pooling layers.\n",
    "2. **Train** on MNIST or CIFAR-10.\n",
    "3. **Log** metrics (accuracy, loss) over epochs.\n",
    "4. **Visualize** sample predictions.\n",
    "\n",
    "### Observations to Note\n",
    "- Did your CNN perform better than the MLP? How quickly?\n",
    "- Common pitfalls: exploding/vanishing gradients, overfitting.\n",
    "\n",
    "---\n",
    "## 4. Thursday: Regularization & Best Practices\n",
    "\n",
    "### Topics:\n",
    "- Techniques to reduce overfitting: **Dropout**, **Batch Normalization**, **Data Augmentation**\n",
    "- **Experiment Tracking**: TensorBoard, W&B, or MLflow\n",
    "\n",
    "### Notebook Tasks:\n",
    "1. **Add dropout** to your CNN and see if it helps.\n",
    "2. **Incorporate batch normalization** layers.\n",
    "3. **Use data augmentation** (random flips, crops, rotations) for CIFAR-10 or MNIST.\n",
    "4. **Set up** a simple experiment tracking method (TensorBoard or other) to compare runs.\n",
    "\n",
    "### Industry Context\n",
    "In real-world projects, controlling overfitting is critical. Data augmentation is standard for image tasks, while logging experiments is vital for systematic iteration.\n",
    "\n",
    "---\n",
    "## 5. Friday: Mini-Project – CNN Showcase\n",
    "\n",
    "### Objective\n",
    "Build a short but comprehensive **CNN project**:\n",
    "1. Load dataset (MNIST, CIFAR-10, or a small custom dataset).\n",
    "2. Define CNN architecture.\n",
    "3. Integrate dropout, batch norm, or data augmentation.\n",
    "4. Train and track your experiments.\n",
    "5. Evaluate final model (accuracy, confusion matrix, sample predictions).\n",
    "6. Summarize your findings.\n",
    "\n",
    "### Key Points\n",
    "- Document everything: hyperparameters, epoch counts, best accuracy, etc.\n",
    "- Reflect on how you might extend this in a real-world scenario (more data, more complex model, etc.).\n",
    "\n",
    "---\n",
    "## 6. Weekend: Review & Consolidation\n",
    "\n",
    "- **Review** each day’s tasks, ensure you’re comfortable with the code and concepts.\n",
    "- If time permits, **refine** your mini-project code for clarity.\n",
    "- **ADHD Tip**: If you feel overwhelmed, break your review into short bursts (Pomodoro style) focusing on one concept at a time.\n",
    "\n",
    "### Next Steps Preview\n",
    "In **Week 4**, we’ll dive deeper into **Advanced Deep Learning** concepts (RNNs, LSTM, Transformers, advanced CNNs/transfer learning).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6deb0a1",
   "metadata": {},
   "source": [
    "## Practical Implementation Sections\n",
    "Below are skeleton code cells and markdown cells for each day. Insert your own details, code, or notes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2daad6",
   "metadata": {},
   "source": [
    "### 1. Monday: Neural Net Fundamentals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecc5de8",
   "metadata": {},
   "source": [
    "**Key Definitions & Intuitive Explanations**\n",
    "- **Perceptron**: *(Fill in your notes: how does it work, what’s the update rule?)*\n",
    "- **Activation Functions**: ReLU, Sigmoid, Tanh (pros/cons, typical usage)\n",
    "- **Forward Pass & Backpropagation**:  *(Write a short summary of chain rule in the context of neural nets.)*\n",
    "- **Gradient Descent**: *(Why do we need it? Basic process?)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f7be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) 1.1 Simple Forward Pass Example\n",
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def simple_forward_pass(inputs, weights, biases):\n",
    "    # Just one hidden layer with ReLU activation\n",
    "    hidden = relu(np.dot(inputs, weights[\"w1\"]) + biases[\"b1\"])\n",
    "    out = np.dot(hidden, weights[\"w2\"]) + biases[\"b2\"]\n",
    "    return out\n",
    "\n",
    "# Example usage:\n",
    "inputs = np.array([1.0, 2.0])\n",
    "weights = {\n",
    "    \"w1\": np.random.randn(2, 3),  # from 2 input neurons to 3 hidden neurons\n",
    "    \"w2\": np.random.randn(3, 1)   # from 3 hidden neurons to 1 output neuron\n",
    "}\n",
    "biases = {\n",
    "    \"b1\": np.random.randn(3),\n",
    "    \"b2\": np.random.randn(1)\n",
    "}\n",
    "\n",
    "output = simple_forward_pass(inputs, weights, biases)\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7b00cd",
   "metadata": {},
   "source": [
    "**Your Observations**:\n",
    "- *(Write notes on how changing `weights` or `biases` might affect the output. How would backprop update these?)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d34bd62",
   "metadata": {},
   "source": [
    "### 2. Tuesday: Framework Setup & MLP for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d88f7",
   "metadata": {},
   "source": [
    "**Instructions**:\n",
    "1. Pick **PyTorch** or **TensorFlow**. Below is a PyTorch example skeleton.\n",
    "2. **Install** if needed: `pip install torch torchvision`.\n",
    "3. **Load MNIST** using `torchvision.datasets.MNIST` or a similar dataset.\n",
    "4. **Define** an MLP model, train it, and track accuracy.\n",
    "5. **Write** your observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16526070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 2.1 PyTorch Setup & MLP Example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 2  # Increase for better results\n",
    "\n",
    "# Data loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # mean and std for MNIST\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define a simple MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "accuracy = 100.0 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb0557a",
   "metadata": {},
   "source": [
    "**Your Observations**:\n",
    "- *(Document final accuracy, how many epochs you trained, any improvements if you tweak hyperparams, etc.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae370db9",
   "metadata": {},
   "source": [
    "### 3. Wednesday: CNN for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1233192c",
   "metadata": {},
   "source": [
    "**Instructions**:\n",
    "1. Build a CNN with convolution + pooling layers.\n",
    "2. Use either MNIST or CIFAR-10 (CIFAR-10 is a bit more challenging).\n",
    "3. Compare CNN accuracy vs. MLP.\n",
    "4. Plot training loss and validation accuracy across epochs (if time).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6904dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 3.1 Example CNN on MNIST (Adjust for CIFAR-10 if you prefer)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64*7*7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64*7*7)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the CNN\n",
    "cnn_model = SimpleCNN()\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=learning_rate)\n",
    "cnn_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Simple training loop (for demonstration; you can refine or add validation)\n",
    "for epoch in range(epochs):\n",
    "    cnn_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        cnn_optimizer.zero_grad()\n",
    "        output = cnn_model(data)\n",
    "        loss = cnn_criterion(output, target)\n",
    "        loss.backward()\n",
    "        cnn_optimizer.step()\n",
    "    print(f\"[CNN] Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate CNN\n",
    "cnn_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = cnn_model(data)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "cnn_accuracy = 100.0 * correct / total\n",
    "print(f\"CNN Test Accuracy: {cnn_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4694bc",
   "metadata": {},
   "source": [
    "**Comparison**:\n",
    "- *(Write how CNN results compare to MLP. Which is higher accuracy? Any interesting patterns?)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ac3c9",
   "metadata": {},
   "source": [
    "### 4. Thursday: Regularization & Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198414b6",
   "metadata": {},
   "source": [
    "**Techniques**:\n",
    "- **Dropout**: random zeroing of some neurons’ outputs.\n",
    "- **Batch Normalization**: normalizes layer inputs, can stabilize training.\n",
    "- **Data Augmentation**: random flips, crops, rotations to artificially expand dataset.\n",
    "- **Experiment Tracking**: TensorBoard, W&B, MLflow.\n",
    "\n",
    "**Notebook Tasks**:\n",
    "1. Implement or add **dropout** in your CNN.\n",
    "2. Add **batch norm** after convolution layers.\n",
    "3. **Augment** data (especially helpful on CIFAR-10) with `transforms.RandomHorizontalFlip()`, etc.\n",
    "4. Use **TensorBoard** or another tool to log accuracy & loss. (Pseudocode or partial code if short on time.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f8b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Example) 4.1 A CNN with Dropout & BatchNorm\n",
    "class CNNWithDropoutBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNWithDropoutBatchNorm, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(64*7*7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64*7*7)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# (Pseudo-code for training this new CNN). You can replicate your training loop from above.\n",
    "# TODO: Compare final accuracy with/without dropout and batchnorm.\n",
    "print(\"Example architecture. You can code the full training & evaluation loop similarly to above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ebe5fc",
   "metadata": {},
   "source": [
    "**Experiment Tracking**:\n",
    "- *(If using TensorBoard, show code to log your loss/accuracy. If using W&B, log in a similar fashion.)*\n",
    "- Summarize the runs in a short markdown cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf27dc9",
   "metadata": {},
   "source": [
    "### 5. Friday: Mini-Project – CNN Showcase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3096b587",
   "metadata": {},
   "source": [
    "**Objective**: Consolidate your CNN knowledge into a short end-to-end project. For example:\n",
    "1. Load & preprocess data.\n",
    "2. Define CNN architecture with dropout/batch norm.\n",
    "3. Train with data augmentation.\n",
    "4. Track experiments (baseline vs. augmented, dropout vs. no dropout).\n",
    "5. Evaluate final results. Show confusion matrix or sample predictions.\n",
    "\n",
    "**Your Steps**:\n",
    "- *(Create code cells that do each step. Document your final results in a markdown cell.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bac51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 5.1 Example skeleton for your mini-project\n",
    "def run_cnn_showcase():\n",
    "    # 1. Data loading & augmentation\n",
    "    # 2. Model definition\n",
    "    # 3. Training loop & experiment tracking\n",
    "    # 4. Evaluation\n",
    "    # 5. Document results\n",
    "    pass\n",
    "\n",
    "print(\"CNN Showcase placeholder. Fill with your own logic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e821b2",
   "metadata": {},
   "source": [
    "### Industry Context\n",
    "CNNs power many real-world applications: image recognition, medical imaging, self-driving car vision, etc. Documenting your mini-project thoroughly demonstrates practical skill.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c0a22",
   "metadata": {},
   "source": [
    "## 6. Weekend: Review & Consolidation\n",
    "\n",
    "### Checklist\n",
    "- [ ] Familiar with forward/backprop?\n",
    "- [ ] Comfortable training MLP & CNN?\n",
    "- [ ] Understanding of overfitting, dropout, batch norm?\n",
    "- [ ] Basic logging of experiments?\n",
    "\n",
    "### ADHD Tip\n",
    "- Break your weekend review into **short, focused sessions**.\n",
    "- Reward yourself after each milestone (completing 1 or 2 bullet points).\n",
    "\n",
    "### Looking Ahead\n",
    "In **Week 4**, we’ll explore **Advanced Deep Learning** topics like RNNs, LSTMs, Transformers, and more advanced CNN or transfer learning methods. Keep your momentum going!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c7aebd",
   "metadata": {},
   "source": [
    "# End of Week 3 Notebook\n",
    "\n",
    "---\n",
    "Congratulations on completing your first steps into Deep Learning! Next week, we’ll expand these fundamentals into more specialized or advanced architectures.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
